name: Semantic Deduplication Benchmarks

on:
  # Run MRPC on pull requests
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'include/**'
      - 'python/**'
      - 'benchmarks/semantic_dedup/**'
      - '.github/workflows/semantic_benchmarks.yml'

  # Weekly full benchmark suite
  schedule:
    - cron: '0 0 * * 0'  # Every Sunday at midnight UTC

  # Manual trigger
  workflow_dispatch:
    inputs:
      datasets:
        description: 'Comma-separated dataset names (default: all)'
        required: false
        default: 'mrpc,qqp,stsb,paws'
      thresholds:
        description: 'Comma-separated thresholds (default: 0.85,0.90,0.95)'
        required: false
        default: '0.85,0.90,0.95'

jobs:
  benchmark:
    name: Run Semantic Dedup Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 120

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            cmake \
            ninja-build \
            librocksdb-dev \
            libonnxruntime-dev \
            pkg-config

      - name: Cache RocksDB build
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/rocksdb
            _deps
          key: ${{ runner.os }}-rocksdb-${{ hashFiles('CMakeLists.txt') }}

      - name: Build Prestige with semantic dedup enabled
        run: |
          cmake -B build -G Ninja \
            -DCMAKE_BUILD_TYPE=Release \
            -DPRESTIGE_ENABLE_SEMANTIC=ON \
            -DPRESTIGE_BUILD_PYTHON=ON \
            -DPRESTIGE_BUILD_TESTS=OFF
          cmake --build build --parallel

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e python/
          pip install -r benchmarks/semantic_dedup/requirements.txt

      - name: Download embedding model
        run: |
          python -m benchmarks.semantic_dedup.models bge-small

      - name: Determine datasets to run
        id: datasets
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # PR: Only run MRPC (fast)
            echo "datasets=mrpc" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            # Weekly: Run all datasets except ParaNMT (not implemented)
            echo "datasets=mrpc,qqp,stsb,paws" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Manual: Use user input
            echo "datasets=${{ github.event.inputs.datasets }}" >> $GITHUB_OUTPUT
          else
            # Default
            echo "datasets=mrpc" >> $GITHUB_OUTPUT
          fi

      - name: Determine thresholds
        id: thresholds
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "thresholds=${{ github.event.inputs.thresholds }}" >> $GITHUB_OUTPUT
          else
            echo "thresholds=0.85,0.90,0.95" >> $GITHUB_OUTPUT
          fi

      - name: Run benchmarks
        run: |
          python -m benchmarks.semantic_dedup.cli run \
            --datasets ${{ steps.datasets.outputs.datasets }} \
            --thresholds ${{ steps.thresholds.outputs.thresholds }} \
            --output ./benchmark_results

      - name: Upload results artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark_results/*.json
            benchmark_results/*.html
          retention-days: 90

      - name: Download baseline (if exists)
        id: baseline
        continue-on-error: true
        run: |
          # Try to download baseline from latest main branch run
          gh run list \
            --workflow=semantic_benchmarks.yml \
            --branch=main \
            --status=success \
            --limit=1 \
            --json databaseId \
            --jq '.[0].databaseId' > baseline_run_id.txt

          if [[ -s baseline_run_id.txt ]]; then
            BASELINE_RUN_ID=$(cat baseline_run_id.txt)
            echo "Found baseline run: $BASELINE_RUN_ID"

            gh run download $BASELINE_RUN_ID \
              --name benchmark-results-$BASELINE_RUN_ID \
              --dir ./baseline_results || true

            if [[ -d ./baseline_results ]]; then
              echo "has_baseline=true" >> $GITHUB_OUTPUT
            else
              echo "has_baseline=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "No baseline run found"
            echo "has_baseline=false" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Compare against baseline
        if: steps.baseline.outputs.has_baseline == 'true' && github.event_name == 'pull_request'
        id: comparison
        continue-on-error: true
        run: |
          for dataset in $(echo "${{ steps.datasets.outputs.datasets }}" | tr ',' ' '); do
            current="./benchmark_results/${dataset}_results.json"
            baseline="./baseline_results/${dataset}_results.json"

            if [[ -f "$current" && -f "$baseline" ]]; then
              echo "Comparing $dataset..."
              python -m benchmarks.semantic_dedup.cli compare \
                "$current" \
                "$baseline" \
                --output "./benchmark_results/${dataset}_comparison.json" || true
            fi
          done

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Read summary
            const summaryPath = './benchmark_results/summary.json';
            if (!fs.existsSync(summaryPath)) {
              console.log('No summary file found');
              return;
            }

            const summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));

            // Build comment body
            let body = '## Semantic Deduplication Benchmark Results\n\n';
            body += '| Dataset | Best F1 | Threshold | ROC AUC | PR AUC |\n';
            body += '|---------|---------|-----------|---------|--------|\n';

            for (const [dataset, results] of Object.entries(summary.datasets)) {
              body += `| ${dataset.toUpperCase()} | ${results.best_f1.toFixed(4)} | ${results.best_threshold.toFixed(2)} | ${results.roc_auc.toFixed(4)} | ${results.pr_auc.toFixed(4)} |\n`;
            }

            body += '\n';

            // Check for regressions
            const comparisonFiles = fs.readdirSync('./benchmark_results')
              .filter(f => f.endsWith('_comparison.json'));

            if (comparisonFiles.length > 0) {
              body += '### Comparison vs Baseline\n\n';

              for (const compFile of comparisonFiles) {
                const comparison = JSON.parse(
                  fs.readFileSync(path.join('./benchmark_results', compFile), 'utf8')
                );

                if (comparison.has_regressions) {
                  body += 'âš ï¸ **Regressions detected:**\n\n';
                  for (const reg of comparison.regressions) {
                    body += `- ${reg.metric}: ${reg.current.toFixed(4)} â†’ ${reg.baseline.toFixed(4)} (${reg.change_pct.toFixed(2)}%)\n`;
                  }
                  body += '\n';
                } else {
                  body += 'âœ… No regressions detected\n\n';
                }

                if (comparison.improvements && comparison.improvements.length > 0) {
                  body += 'âœ¨ **Improvements:**\n\n';
                  for (const imp of comparison.improvements) {
                    body += `- ${imp.metric}: ${imp.baseline.toFixed(4)} â†’ ${imp.current.toFixed(4)} (+${imp.change_pct.toFixed(2)}%)\n`;
                  }
                  body += '\n';
                }
              }
            }

            body += '\nðŸ“Š Full HTML reports available in workflow artifacts.\n';

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Save baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
        run: |
          echo "Results from main branch will be used as baseline for future PRs"
          # Results are already uploaded as artifacts and can be downloaded by future runs
